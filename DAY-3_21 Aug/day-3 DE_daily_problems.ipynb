{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d85d75-043d-4b71-939e-a67cf5985cfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+\n",
      "|customer_id|purchase_date|amount|\n",
      "+-----------+-------------+------+\n",
      "|        101|   2025-01-03|   250|\n",
      "|        102|   2025-01-01|   150|\n",
      "|        103|   2025-01-04|   500|\n",
      "+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"purchase_date\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (101, \"2025-01-03\", 250),\n",
    "    (101, \"2025-01-05\", 300),\n",
    "    (102, \"2025-01-01\", 150),\n",
    "    (102, \"2025-01-02\", 200),\n",
    "    (103, \"2025-01-04\", 500),\n",
    "]\n",
    "\n",
    "ddf = spark.createDataFrame(data, schema)\n",
    "# ddf.show()\n",
    "\n",
    "window_date = Window.partitionBy('customer_id').orderBy('purchase_date')\n",
    "\n",
    "df_date = ddf.withColumn(\"rn\", row_number().over(window_date)) \n",
    "# df_date.show()\n",
    "ddf_date = df_date.filter(col(\"rn\")==1).select('customer_id', 'purchase_date', 'amount')\n",
    "ddf_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d31e59b-fc57-4ea9-8c0d-924f1f22eacc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+------+-------+-------+\n",
      "|emp_id|effective_date|salary|lag_sal|changed|\n",
      "+------+--------------+------+-------+-------+\n",
      "|     1|    2025-01-01| 50000|   null|      0|\n",
      "|     1|    2025-02-01| 55000|  50000|      1|\n",
      "|     1|    2025-03-01| 60000|  55000|      1|\n",
      "|     2|    2025-01-15| 40000|   null|      0|\n",
      "|     2|    2025-03-01| 45000|  40000|      1|\n",
      "|     3|    2025-01-10| 30000|   null|      0|\n",
      "+------+--------------+------+-------+-------+\n",
      "\n",
      "+------+-------------------+\n",
      "|emp_id|salary_change_count|\n",
      "+------+-------------------+\n",
      "|     1|                  2|\n",
      "|     2|                  1|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"effective_date\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"2025-01-01\", 50000),\n",
    "    (1, \"2025-02-01\", 55000),\n",
    "    (1, \"2025-03-01\", 60000),\n",
    "    (2, \"2025-01-15\", 40000),\n",
    "    (2, \"2025-03-01\", 45000),\n",
    "    (3, \"2025-01-10\", 30000),\n",
    "]\n",
    "\n",
    "df_emp = spark.createDataFrame(data, schema)\n",
    "# df_emp.show()\n",
    "\n",
    "window_emp = Window.partitionBy('emp_id').orderBy('effective_date')\n",
    "\n",
    "ddf_emp = df_emp.withColumn('lag_sal', lag('salary').over(window_emp))\n",
    "# ddf_emp.show()\n",
    "ddf_emp1 = ddf_emp.withColumn(\n",
    "    \"changed\",\n",
    "    when(col(\"salary\") != col(\"lag_sal\"), 1).otherwise(0)\n",
    ")\n",
    "ddf_emp1.show()\n",
    "\n",
    "ddf_emp2 = ddf_emp1.groupBy('emp_id').agg(sum(col('changed')).alias('salary_change_count')).filter(col('salary_change_count')>=1)\n",
    "ddf_emp2.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2759180062096013,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "day-2 DE_daily_problems",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
